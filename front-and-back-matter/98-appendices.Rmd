`r if(knitr:::is_latex_output()) '\\startappendices'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!-- If you feel it necessary to include an appendix, it goes here. The first appendix should include the commands above. -->

# Backpropagation with Binary Cross-Entropy {#chap:appendix_a}
Following calculations are heavily inspired from these notes^[https://www.ics.uci.edu/~pjsadows/notes.pdf]. The notation and the general structure might be different, but the idea is the same.\
Let us consider a simple binary classification task. It is common to use a network with a single logistic output with the binary cross-entropy loss function and for the sake of simplicity, let us assume that there is only one hidden layer.
$$
\begin{aligned}
BCE=-\sum_{i=1}^{n o u t}\left(y_i \log \left(\hat{y}_i \right)+\left(1-y_i\right) \log \left(1-\hat{y}_i\right)\right)
\end{aligned}
$$

\noindent Where $y$ is the ground truth and $\hat{y}$ is the output of the network. After having the loss function, let us continue with the forward pass.

$$
\begin{aligned} 
a_{k} &= h_{k-1} w_{k} + b_k \\
h_k &= f(a_{k})
\end{aligned}
$$

\noindent Where, $w_k$ is the weight, $b_{k}$ is the bias term, $h_k$ is the output of the layer (which means that $h_0 = X$ and $h_2 = \hat{y}$) and f is the non linear function. Please note that for last layer logistic function is used whereas for hidden layer reLU is used as non linear functions.\
We can compute the derivative of the weights by using the chain rule.

$$
\begin{aligned} 
\frac{\partial BCE}{\partial w_{2}}=\frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial w_{2}}
\end{aligned}
$$

\noindent Computing each factor in the term, we have:
$$
\begin{aligned}
\frac{\partial BCE}{\partial \hat{y}} &=\frac{-y}{\hat{y}}+\frac{1-y}{1-\hat{y}} \\
&=\frac{\hat{y}-y}{\hat{y}\left(1-\hat{y}\right)} \\
\frac{\partial \hat{y}}{\partial a_{2}} &=\hat{y}\left(1-\hat{y}\right) \\
\frac{\partial a_{2}}{\partial w_{2}} &=h_{1}^T
\end{aligned}
$$
This expression gives us:
$$
\frac{\partial BCE}{\partial w_{2}}=h_{1}^T\left(\hat{y}-y\right)
$$
We can calculate the derivative of the $w_1$ concerning loss function as the following:

$$
\begin{aligned} 
\frac{\partial BCE}{\partial w_{1}}=\frac{\partial BCE}{\partial h_1} \frac{\partial h_1}{\partial a_{1}} \frac{\partial a_{1}}{\partial w_{1}}
\end{aligned}
$$
Compute each factor in the term again, we have:

$$
\begin{aligned}
\frac{\partial BCE}{\partial h_1} &= \frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial h_{1}}  \\
&= \left(\hat{y}-y\right) w_{2}^T \\
\frac{\partial h_1}{\partial a_{1}} &=f'(a_1) \\
\frac{\partial a_{1}}{\partial h_{1}} &=X^T
\end{aligned}
$$
This expression gives us:
$$
\begin{aligned}
\frac{\partial BCE}{\partial w_{1}}= \left(X\right)^T\left(\hat{y}-y\right)\left(w_{2}^T\right) \odot f'(a_1)
\end{aligned}
$$
Where $\odot$ is element-wise multiplication, similarly, bias terms can be calculated by following:

$$
\begin{aligned} 
\frac{\partial BCE}{\partial b_{2}}&=\frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial b_{2}} \\
&= \left(\hat{y}-y\right)
\end{aligned}
$$

$$
\begin{aligned} 
\frac{\partial BCE}{\partial b_{1}}&=\frac{\partial BCE}{\partial h_1} \frac{\partial h_1}{\partial a_{1}} \frac{\partial a_{1}}{\partial b_{1}} \\
&= \left(\hat{y}-y\right)\left(w_{2}^T\right) \odot f'(a_1)
\end{aligned}
$$
After having all these results, we can update the parameters (weights and biases) using gradient descent and its variants.

# Hidden Representation of Digits in MNIST-Parity Problem
One of the interesting questions is: Does the network learn the digits individually when we train the network for the MNIST-Parity task and make the necessary processes (summation, division), or does it memorize the way data is fed? We can answer this question by observing the hidden representation. If the network learns the digits well, we need to observe a good separation like the standard MNIST task (classifying hand digits). For this purpose, hidden representations of the networks trained with BP and DFA are plotted in two-dimensional space by using t-SNE \cite{vanDerMaaten2008} using the implementation of sklearn \cite{scikit-learn}.\
The hidden representation of a single image in the $k=3$ case is obtained in the following way; we know that after we flatten the images, particular parts of each image are multiplied by corresponding parts of the weight matrix. Getting these parts and performing multiplication for each digit will reveal the hidden representation of the individual digit trained in the $k=3$ case. Process is visualized in \ref{fig:HiddenRepProcess}. After extracting the pixels of each image and the corresponding weights, we end up with the following matrix multiplication: $[\text{Batch Size} \times 784] \dot [784 \space \times \space \text{Hidden Layer}]$ where $784$ is $28 \times 28$ and $28$ is the pixel size of a single digit. Since the t-SNE is a computationally expensive process, only 7500 random samples are plotted from the dataset.
```{r HiddenRepProcess, fig.scap="Process of Extracting the Hidden Representation of Single Digit ", fig.cap="\\textbf{Process of Extracting the Hidden Representation of Single Digit } \\newline After extracting the digits and the corresponding weights. The matrix multiplication (same color) for each digit are performed separately; by doing so, we have the hidden representation of a single image from the network trained for parities of the sum of three digits.", fig.align='center', out.width='100%', echo=FALSE} 
knitr::include_graphics("figures/B_hidden_rep_process.png")
```
\noindent With this matrix multiplication, we have the hidden representation of each image. The rest is visualizing them by using the t-SNE. In figure \ref{fig:tSNEBPDFA} we can observe the results in two dimensions and say that the networks do not capture the information about the digits individually when they are trained for parities of the sum of three digits. They learn different properties of the data, not the digits individually. However, for the same purpose, transfer learning can be implemented to test if these networks trained for the parity problem can classify the digits accurately. Transfer learning is a method that involves storing the information gained while solving a problem and using this information on a different, but related problem \cite{DBLP:journals/corr/JhaS15}. On the other hand, it is not surprising that BP and DFA capture very similar information from the dataset.
```{r tSNEBPDFA, fig.scap="Hidden Representation of Digits in BP and DFA", fig.cap="\\textbf{Hidden Representation of Digits in BP and DFA} \\newline Hidden representations of images with their labels for BP and DFA. The process of acquiring these representations are explained in \\ref{fig:HiddenRepProcess}.", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/B_tsne_BPDFA.png")
```

# Reproducibility
For reproducing the experiment results, please refer to this Github repository^[https://github.com/demirbilek95/Dynamics-of-Learning]. It contains all the codes that are used to have the experiment results. It also includes the requirements to run the code. However, due to the stochastic behavior of the neural networks and the data, results will not be the same. Nevertheless, on average, it must be very close to the ones that are presented. The neural network weights are sampled from a uniform distribution so that each instance will be slightly different. Given the parameter $k$, the parity data is sampled uniformly from MNIST and horizontally stacked. In addition to this, at each epoch, it is recreated, and we had a different dataset. The synthetic data also follows stochastic behavior. The labels of the clusters are assigned randomly, and the cluster samples are distributed from a uniform distribution (it is explained detailly in \ref{chap:chapter_2}). The distribution of the first two dimensions is plotted with experiments. Other distributions would give different results. Hyperparameters used in the experiments can be found in the Github repository, and the process of how they are tuned or which value they are set is explained in the related chapters.\
Lastly, a local computer is used for all the experiments, and all the experiments are performed on GPU (Nvidia GTX 960M).

