---
output:
  pdf_document: default
  html_document: default
---

`r if(knitr:::is_latex_output()) '\\startappendices'`

<!-- `r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'`  -->

<!-- If you feel it necessary to include an appendix, it goes here. The first appendix should include the commands above. -->

# Backpropagation with Binary Cross-Entropy {#chap:appendix_a}
Let's consider a simple binary classification task. It is common to use a network with a single logistic output with the binary cross-entropy (BCE) loss function and for the sake of simplicity, let's assume that there is only one hidden layer.
$$
\begin{aligned}
BCE=-\sum_{i=1}^{n o u t}\left(y_i \log \left(\hat{y}_i \right)+\left(1-y_i\right) \log \left(1-\hat{y}_i\right)\right)
\end{aligned}
$$

\noindent Where $y$ is the ground truth and $\hat{y}$ is the output of the network. After having the loss function, let's continue with the forward pass.

$$
\begin{aligned} 
a_{k} &= h_{k-1} w_{k} + b_k \\
h_k &= f(a_{k})
\end{aligned}
$$

\noindent Where, $w_k$ is the weight, $b_{k}$ is the bias term, $h_k$ is the output of the layer (which means that $h_0 = X$ and $h_2 = \hat{y}$) and f is the non linear function. Please note that for last layer logistic function is used whereas for hidden layer reLU is used as non linear functions.\
We can compute the derivative of the weights by using the chain rule.

$$
\begin{aligned} 
\frac{\partial BCE}{\partial w_{2}}=\frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial w_{2}}
\end{aligned}
$$

Computing each factor in the term, we have:
$$
\begin{aligned}
\frac{\partial BCE}{\partial \hat{y}} &=\frac{-y}{\hat{y}}+\frac{1-y}{1-\hat{y}} \\
&=\frac{\hat{y}-y}{\hat{y}\left(1-\hat{y}\right)} \\
\frac{\partial \hat{y}}{\partial a_{2}} &=\hat{y}\left(1-\hat{y}\right) \\
\frac{\partial a_{2}}{\partial w_{2}} &=h_{1}^T
\end{aligned}
$$
This gives us:
$$
\frac{\partial BCE}{\partial w_{2}}=h_{1}^T\left(\hat{y}-y\right)
$$
We can calculate the derivative of the $w_1$ concerning loss function as the following:

$$
\begin{aligned} 
\frac{\partial BCE}{\partial w_{1}}=\frac{\partial BCE}{\partial h_1} \frac{\partial h_1}{\partial a_{1}} \frac{\partial a_{1}}{\partial w_{1}}
\end{aligned}
$$
Compute each factor in the term again, we have:

$$
\begin{aligned}
\frac{\partial BCE}{\partial h_1} &= \frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial h_{1}}  \\
&= \left(\hat{y}-y\right) w_{2}^T \\
\frac{\partial h_1}{\partial a_{1}} &=f'(a_1) \\
\frac{\partial a_{1}}{\partial w_{1}} &=X^T
\end{aligned}
$$
This gives us:
$$
\begin{aligned}
\frac{\partial BCE}{\partial w_{1}}= \left(X\right)^T\left(\hat{y}-y\right)\left(w_{2}^T\right) \odot f'(a_1)
\end{aligned}
$$
Where $\odot$ is element-wise multiplication, similarly, bias terms can be calculated by following:

$$
\begin{aligned} 
\frac{\partial BCE}{\partial b_{2}}&=\frac{\partial BCE}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial a_{2}} \frac{\partial a_{2}}{\partial b_{2}} \\
&= \left(\hat{y}-y\right)
\end{aligned}
$$

$$
\begin{aligned} 
\frac{\partial BCE}{\partial b_{1}}&=\frac{\partial BCE}{\partial h_1} \frac{\partial h_1}{\partial a_{1}} \frac{\partial a_{1}}{\partial b_{1}} \\
&= \left(\hat{y}-y\right)\left(w_{2}^T\right) \odot f'(a_1)
\end{aligned}
$$
After having all these results, we can update the parameters (weights and biases) using gradient descent and its variants.

# Hidden Representation of Digits in Parity Problem
One of the interesting questions is: does the network learn the digits individually when we train it and make the necessary processes (summation, division), or does it memorize the way data is fed? We can answer this question by observing the hidden representation. If the network learns the digits well, we need to observe a good separation like the MNIST task. For this purpose, hidden representations of the networks trained with BP and DFA are plotted in two-dimensional space by using t-SNE \cite{vanDerMaaten2008} implementation of sklearn \cite{scikit-learn}.\
The hidden representation of a single image in $k=3$ case is obtained in the following way; we know that after we flatten the images, particular parts of each image are multiplied by corresponding parts of the weight matrix. Getting these parts and performing multiplication for each digit will give us the hidden representation of the individual digit trained in $k=3$ case. Process is visualized in \ref{fig:HiddenRepProcess}. Since the t-SNE is a computationally expensive process, only 7500 random samples are plotted from the dataset.
```{r HiddenRepProcess, fig.cap="Process of Hidden Representation", fig.align='center', out.width='60%', echo=FALSE}
knitr::include_graphics("figures/B_hidden_rep_process.png")
```
\noindent With this matrix multiplication, we have the hidden representation of each image. The rest is visualizing them by using the t-SNE. In figure \ref{fig:tSNEBPDFA} we can observe the results in two dimensions and say that the networks do not capture the information about the digits individually. They learn by memorizing the samples, not the digits.
```{r tSNEBPDFA, fig.cap="Hidden Representation of Digits in BP and DFA", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/B_tsne_BPDFA.png")
```

# Reproducibility
TODO: Write down the library versions, refer to the Github repository for the experiments, give information about the hardware. Explain the stochastic behavior of data.



