Backpropagation is the main algorithm to train neural networks; however, it is not the only alternative. Different algorithms propose more biologically plausible solutions. Here, we perform various experiments on particular problems by using one of the alternative algorithm called direct feedback alignment among backpropagation. The problems under consideration were used in the previous studies showed the superiority of the backpropagation over the lazy methods. Our main aim is to observe the predictive power of the direct feedback alignment on the same problems and improve its performance with tuned hyperparameters and modern adaptation techniques. Besides, we perform side experiments that help to understand the dynamics of learning of both methods. We discover that direct feedback alignment can perform equally well as backpropagation on the chosen task with sensitive tuning and suitable modern techniques. We hope that our study would inspire to test direct feedback alignment from various perspectives on more challenging problems.