---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Experiments {#chap:chapter_3}

\noindent After having the previous study results, we can continue to test DFA on the learning problems. Train phase and the hyperparameter tuning process are explained in chapter \ref{chap:chapter_2}. These processes are the same for the following experiments. For all experiments, scratch implementations are used with minimal Pytorch functionalities. They are performed three times and plotted with their mean and standard deviation or with their confidence interval.\
The same architectures are used as the previous studies for BP and DFA, meaning that we have only a hidden layer with 512 neurons for both problems, and reLU is used as a non-linear function for the hidden layer. BCE is chosen as a loss function and, sigmoid is preferred for the non-linearity of the last layer. For the parity problem, networks are trained for 20 epochs unless others are specified, and at each epoch, train and test datasets are recreated as it is explained in chapter \ref{chap:chapter_2}. For the synthetic data problem, networks are trained for 500 epochs. Weights of the networks are initialized uniformly with $\frac{1}{\sqrt{input dim}}$ as in default Pytorch weight initialization. Moreover, the random matrix $B$ is initialized with the same way to have similar behaviors as the weight matrix unless other specified.

## Parity Learning Experiments
For each experiment, when there is a change in the training method (DFA or BP) or optimization method, hyperparameter tuning is performed to obtain a decent learning rate. Weight decay is set to $10^{-3}$ for each experiment. Since the experiment details are precise, we can test DFA on the parity learning problem with BP using SGD.
```{r BPvsDFA, fig.scap="BP and DFA on MNIST-Parity Problem with SGD", fig.cap=" \\textbf{BP and DFA on MNIST-Parity Problem with SGD} \\newline The predictive power of DFA on MNIST-Parity experiment among BP. The left is the test accuracy for the parity of a single image. The right is the test accuracy for the parity of three images." , fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_k13_SGD_DFAvsBP.png")
```
\noindent We can observe the results in \ref{fig:BPvsDFA} with $95\%$ confidence interval. In the case $k=1$, although DFA outperforms the lazy methods, it is behind the BP. In the case $k=3$, it is obvious that DFA performs much better than lazy methods. However, the gap between BP and DFA is a bit higher than the $k=1$ case. It seems like there is a limit for DFA to reach with SGD that is $\%70$. The reason is that DFA has an additional task to accomplish, which is aligning with BP's teaching signals. In other words, the network loses time while trying to make teaching signals useful. This delays the convergence and causes performance lag. We can see that during the first iterations, DFA does not converge fast enough to catch up with BP, and it always stays behind it.\
The thrilling question is, is there a performance limit for DFA to reach, and can we get a similar performance as BP by making some changes? For answering the first question, it is better to run DFA for more epochs to see if it can reach a similar performance as BP. Because with longer training, DFA will have time to align and converge. It would be convenient to test DFA with different random matrices to observe any improvement for the second question. Because it is clear that learning in DFA is strongly dependent on random matrix. Besides, it is interesting to test if DFA can learn with a different type of random matrices. While tuning the learning rate for DFA, we noticed that it is susceptible to the learning rate. Low learning rates caused no convergence, and larger learning rates showed over-fitting within specified epoch numbers. Therefore we can use adaptive methods to have better convergence properties both in BP and DFA. These methods are specifically good at adjusting the learning rate, which is more difficult to tune for DFA than BP.\
\noindent We trained DFA for 50 epochs with a tuned learning rate to observe if it can reach a similar performance as BP. At the same time, alignment between the random matrix and the transpose of the weight matrix is plotted. This alignment is measured by using the cosine similarity.
```{r DFA50epochs, fig.scap="DFA on MNIST-Parity Problem with Alignment", fig.cap="\\textbf{DFA on MNIST-Parity Problem with Alignment} \\newline DFA on MNIST-Parity task for the case $k=3$ trained $50$ epochs. The left is the test accuracy of DFA on the MNIST-Parity task for the case $k=3$ trained for $50$ epochs. The right is the cosine similarity of the random matrix and the transpose of the weight matrix.", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_k3_SGD_DFA_Alingment.png")
```
\noindent From figure \ref{fig:DFA50epochs}, we can see that DFA can reach a similar performance as BP trained with SGD. This result approves our comments about the additional task DFA has and why it takes longer to achieve the same performance. On the right side of the plot, we can examine the alignment between the random matrix and the transpose of the weight matrix. At the beginning of training, the similarity is low. However, with advancing steps, we can see that alignment becomes higher, similar to the performance. It shows that the network aligns with the BP's teaching signals. In other words, the network learns how to learn by using the random matrix.
\noindent After having a similar performance from DFA with SGD, it is intriguing to test if we can achieve similar performance within the same epoch number. For this purpose, the first improvement attempt will be related to random matrices. Using different random matrices may influence the performance of DFA. Some of them might align better with BP's teaching signals. On the other hand, it is interesting to observe if we can learn with any random matrix.\
Apart from uniform random matrix, three different random matrices are tested. They are initialized as the following: \textbf{standard uniform} is default Pytorch initialization that is uniformly distributed from $0$ to $1$. \textbf{Gaussian} is initialized normally with $\mu$ and $\sigma$ are equal to each other that is $\frac{1}{\sqrt{input dim}}$. Lastly, \textbf{standard gaussian} is initialized with $\mu=0$ and $\sigma=1$.
```{r DFARandomMatrices, fig.scap="DFA on MNIST-Parity Problem with Various Random Matrices", fig.cap="\\textbf{DFA on Parity Problem with Various Random Matrices} \\newline The final test accuracy of DFA on the MNIST-Parity for the case $k=3$. The experiments performed three times and plotted with $95\\%$ confidence interval. The random matrices are initialized as the following: $\\text{standard uniform} \\sim U(0,1)$, $\\text{uniform} \\sim U(-a,+a)$, $\\text{standard gaussian} \\sim \\mathcal{N}\\left(0, 1\\right)$ and $\\text{gaussian} \\sim \\mathcal{N}\\left(a, a \\right)$ where $a= \\frac{1}{\\sqrt{input dim}}$.", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_k3_DFA_RandomMatrices.png")
```
\noindent From figure \ref{fig:DFARandomMatrices}, we can observe that DFA can learn with any random matrices. However, it is essential to specify that learning rates for each random matrix are tuned and drastically different. Apart from standard uniform, the rest of the random matrices achieved similar performances, but they are still behind the BP. However, thanks to these results, we can see that DFA is highly sensitive to the learning rate. Because during the tuning phase, small learning rates did not converge within the specified epoch number. On the other hand, high learning rates demonstrated overfitting for each random matrices. Therefore, since adaptive methods have better convergence properties, they may increase the performance of DFA as they did in BP. For the rest of the DFA experiments, the random matrix is uniformly initialized since there is no significant improvement with other initializations.\
\noindent Following the previous deduction, various adaptive methods are tested on the parity learning problem for BP and DFA. Their learning rates are tuned, as it is explained in the previous chapter. For the experiments, they are run three times, and their final test accuracies are plotted. The results are presented in figure \ref{fig:mainExperiment}.
```{r mainExperiment, fig.scap="DFA and BP on MNIST-Parity Problem with Adaptive Methods", fig.cap="\\textbf{DFA and BP on MNIST-Parity Problem with Adaptive Methods} \\newline The final test accuracies for BP and DFA with adaptive optimization algorithms. The experiments were performed three times and plotted with a $95\\%$ confidence interval.", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_mainExperiment.png")
```
\noindent As expected, adaptive methods improve the final test accuracy significantly for both BP and DFA. On average, DFA is still behind the BP, but with RMSProp and Adadelta, the gap is much smaller than with plain SGD. Sometimes DFA's final test accuracy even exceeds BP. In other words, we can say that some adaptive methods help DFA more than BP. However, we should not ignore that DFA has larger fluctuations for the final test accuracy than BP. Thanks to adaptive methods, the last experiment could close the gap between BP and DFA for the parity learning problem.
\noindent The reason for the improvement of adaptive methods in DFA is an excellent question to investigate the algorithm's behavior. One possible idea is, adaptive methods may spawn better alignment than SGD. Testing this theory is relatively easy. We can train DFA with SGD and one of the adaptive methods, and then we can observe the alignment of the random matrix and the transpose of the weight matrix with gradient alignments.
```{r Alignment, fig.scap="Alignment Comparison of SGD and RMSProp", fig.cap="\\textbf{Alignment Comparison of SGD and RMSProp} \\newline The top is the alignment of the random matrix and the transpose of the weight matrix for SGD and RMSProp. The bottom is the alignment of the gradients of $w_1$ concerning loss FOR SGD and RMSProp. Both plots are trained on the MNIST-Parity task for the case $k=3$ for $50$ epochs." ,fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_k3_SGD_RMSProp_DFA_Alingment.png")
```
\noindent In figure \ref{fig:Alignment}, we can observe the alignment measures of SGD and RMSProp. It is interesting to point out that SGD has better alignment during the later training steps than RMSProp. From the results of the previous experiments, we know that RMSProp performed better than SGD on this task. Given that, for having better performance, alignment is not always required. On the other hand, at the beginning of the training, RMSProp aligns faster than SGD. Therefore, we can say that there is a faster alignment with adaptive methods at the beginning that prevents DFA from losing time at first epoch numbers, but later DFA finds other paths to find the minimum that is different from BP. These outcomes are parallel to results of \cite{refinetti2021align}, it was stated that DFA first aligns with BP, and later it sacrifices from this alignment to find better paths to the minimum. This phenomenon happens quicker with adaptive methods than plain SGD.

## Synthetic Data Experiments
After having the previous experiment result and current experiments details for synthetic data, we can test the DFA on the number of dimensions plots with the test error to observe the difference between BP. Most of the parameters are the same as MNIST-Parity experiments. The only differences are the learning rate and the weight decay. The learning rate is set to $0.5$ without hyperparameter tuning for all methods, and weight decay is not used. They are performed with three repetitions, and they are plotted with their $95\%$ confidence interval.
```{r DFARandom, fig.scap="BP and DFA on Synthetic Data Problem with SGD", fig.cap="\\textbf{BP and DFA on Synthetic Data Problem with SGD} \\newline On the left, the distribution of the input of two first dimensions is used for the experiment. On the right, test error of BP and DFA with SGD and RMSProp as a function of the number of dimensions where $n = 256$." ,fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/3_syntheticData_adaptive.png")
```
\noindent Before interpreting the results, it is essential to mention that experiment results will be drastically different if the input distribution has changed. We have chosen a distribution that shows the difference among the algorithms well. It would be more convenient to run for different datasets with more repetition and plot the average. We left this for future considerations.
\noindent We can observe the results in \ref{fig:DFARandom}. DFA performs better than the lazy method. However, BP outperforms the DFA. Since it is natural to try adaptive methods to hope for an improvement, as we experienced for the parity experiments, we used RMSProp on the same problem both for BP and DFA. It again increases the predictive power of DFA, but it is still behind the BP with SGD. It is also interesting that BP with RMSprop did not perform better than plain BP with SGD.
Furthermore, it is promising to see that DFA improves with one adaptive method for another challenging problem. We have to specify that,  It would be intriguing to observe well-tuned DFA with different adaptive methods on the same problem; this is again left for future investigations.\

\noindent \textbf{Highlights} \
With the synthetic data experiment, we conclude the experiments. We started with testing the predictive power of DFA on the MNIST-Parity task. Then, we visualized the alignment of the random matrix and the transpose of the weights. After that, we tried to improve the performance of DFA by trying different types of random matrices, and we used adaptive methods. We discovered that with the help of adaptive methods, DFA could perform as well as BP within the same epoch numbers. For investigating why adaptive methods help DFA more than BP, we compared the alignments of SGD and RMSProp for DFA. We concluded that better alignment does not always mean better performance. Lastly, we performed the synthetic data experiment using a fixed distribution. We plotted the final test error of BP and DFA, both with SGD and RMSProp as the function of the number of dimensions. Once again, we noticed that adaptive methods boost the performance of DFA. We left some further experiments for future studies.