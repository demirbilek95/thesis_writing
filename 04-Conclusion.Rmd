---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Conclusion {#conc -}
This research aimed to compare DFA with BP on the particular learning problems that proved the superiority of the BP over the linear methods. Moreover, we tried to improve their predictive power on these learning problems. In doing so, we hoped to understand their learning dynamics better.\
Thanks to the error propagation mechanism of DFA, it proposes a more biologically plausible algorithm than BP. However, the performance is still an enormous part of the success of the algorithms. Furthermore, based on the theoretical foundations of the algorithms that we presented in chapter \ref{chap:chapter_1}, it was expected to experience performance lag for DFA behind BP. For validating this empirically, we used a set of learning problems that are explained in chapter \ref{chap:chapter_2}. In previous studies, these problems proved the dominance of the BP over the lazy methods. After reproducing the previous experiments from these studies, we tested DFA on the same problems. Our expectations were validated with the initial results. These results are demonstrated in chapter \ref{chap:chapter_3} among other experiments.  Although DFA performed better than the linear methods, there was a performance gap between plain DFA and BP without any intervention. Then, we tried to close this gap by performing a few modifications either on the training algorithm or the optimization process. Even though these modifications did not constantly improve the performance, we acquired a deeper understanding of algorithms' learning behaviors. They opened the gate of correct interventions to close the gaps. Finally, these interventions yield that using adaptive optimizers, DFA could perform equal or close to BP's predictive performance on the particular learning problems that the lazy methods notoriously failed or did not perform well.\
Although we performed various experiments that investigate the performance and learning behaviors of the algorithms, there are still too many aspects that need to be investigated further. For instance, we used architectures with only a hidden layer, and layer size was $512$; so the network's depth and the layer's width are not investigated further. Different activation functions are not tested apart from reLU. It would be intriguing to observe the behavior of DFA with these aspects. Also, one of the experiments that analyze the hidden representation of the digits in the parity learning problem can be extended. For instance, transfer learning can be implemented to test if networks trained for the parity problem can classify the digits accurately. In other words, the network trained for the parity learning problem can be tested to classify the digits. Although these hidden representations demonstrated the opposite, one might find a trace of learning of the digits. In addition to these propositions, the learning problems can constantly be enriched, and new challenging problems can be considered with DFA. Also, different alternative algorithms can be tested on these challenging problems. Furthermore, all the theoretical and empirical aspects of the learning problems we use were not analyzed. For instance, we used a fixed distribution for the synthetic data problem. One might want to use more distributions and observe the average of the predictive powers of the algorithms. Moreover, for the synthetic data experiment, we observed that adaptive methods have a different effect on DFA and BP. Understanding the reason behind this is an attractive path to pursue. All these kinds of further experiments are left for future studies.\
BP is the touchstone for training neural networks. Although the community demonstrated gigantic advancements for optimization methods or architecture types of the networks, BP is still around for decades, and the algorithm's success is indisputable. It is vital to point out that the advancement of BP did not happen in one day. DFA and other alternative algorithms bring a new perspective on the training phase of the networks. With this, we can remove some of the restrictions enforced by BP, such as biological implausibility, lack of parallelism, et cetera. We believe that testing the different aspects of the alternative algorithms and comparing them with BP would contribute a lot to the success of deep learning. These studies might spawn more robust training algorithms which obey the biological plausibility, and they might even train faster with a performance close or equal to BP.\    