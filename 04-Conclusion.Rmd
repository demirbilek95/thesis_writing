---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Conclusion {#conc -}
This research aimed to observe and improve the DFA's performance on the problems that proved the superiority of the BP over the lazy methods. In doing so, we hoped to understand better their learning dynamics with this set of experiments with others.\
Thanks to the error propagation mechanism of DFA, it proposes a more biologically plausible algorithm than BP. However, the performance is still an enormous part of the success of the algorithms. Furthermore, based on the theoretical foundations of the algorithms that we presented in chapter \ref{chap:chapter_1}, it was expected to experience performance lag for DFA behind BP. For validating this empirically, we used a set of problems that are explained in chapter \ref{chap:chapter_2}. These problems proved the dominance of the BP over the lazy methods. After reproducing the previous experiments from these studies, we tested DFA on the same problems. Our expectations were validated with the initial results. These results are demonstrated in chapter \ref{chap:chapter_3} among other experiments.  Although DFA performed better than the lazy methods, there was a performance gap between plain DFA and BP without any intervention. Then, we tried to close this gap by performing a few modifications either on the training algorithm or the optimization process. Even though these modifications did not constantly improve the performance, we acquired a deeper understanding of algorithms' learning behaviors. They opened the gate of correct interventions to close the gaps. Finally, these interventions yield that using adaptive optimizers, DFA could perform equally well as BP on the particular problems that the lazy methods notoriously failed.\
Although we performed various experiments that investigate the performance and learning behaviors of the algorithms, there are still too many aspects that need to be investigated further. For instance, the network's deepness and the layers' width are not investigated further, and different activation functions are not tested apart from reLU. It would be intriguing to observe the behavior of DFA with these aspects. Also, one of the side experiments that analyze the hidden representation of the digits in the parity learning problem can be extended with transfer learning. In other words, the network trained for the parity learning problem can be tested to classify the digits. Although these hidden representations demonstrated the opposite, one might find a trace of learning of the digits. In addition to these propositions, the learning problems can constantly be enriched, and new problems can be considered with DFA. Also, not all the theoretical and empirical aspects of the problems at hand are analyzed. Hence, theoretical explanations and other experiments are left for future studies.\
BP is the touchstone for training neural networks. Although the community demonstrated gigantic advancements for optimization methods or architecture types of the networks, BP is still around for decades, and the algorithm's success is indisputable. DFA and other alternative algorithms bring a new perspective on the training phase of the networks. With this, we can remove some of the restrictions enforced by BP, such as biological plausibility, parallelism, faster training et cetera. It is vital to point out that the advancement of BP did not happen in one day. Hence, we believe that testing the different aspects of the alternative algorithms and comparing them with BP would contribute a lot to the success of deep learning. These studies might spawn more robust training algorithms which obey the biological plausibility, and they might even train faster with a performance close or equal to BP.\    