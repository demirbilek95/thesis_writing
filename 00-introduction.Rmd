---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Introduction {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Introduction}{}
```

Artificial neural networks (ANNs) are a collection of connected computational nodes which is inspired by biological neural networks, each connection can transmit a helpful signal to another computational node like synapses in a brain. ANNs demonstrated colossal advancements in the last decades, thanks to these advancements, it is possible to solve complex problems like image classification, video generation, speech recognition, and machine translation within a reasonable amount of time and with satisfactory performance. These advancements were actualized through an old, but powerful algorithm called backpropagation (BP). BP is a training algorithm for ANN which is based on adjusting network weights repeatedly to minimize a measure of the difference or in other words loss, between the output of the network and the ground truth \cite{Rumelhart:1986we}. \
Although nowadays BP is the workhorse algorithm for ANNs, it has some drawbacks and it is not the only alternative. Recent studies offered different algorithms to train ANNs by addressing these drawbacks, these algorithms have different properties and principles than BP. Some of them are compatible with BP or they even outperform the BP in terms of performance or convergence speed for specific problems. \
This thesis investigates the learning structures through BP and one of the alternative algorithm called direct feedback alignment (DFA) on the particular problem. In DFA, unlike BP error is propagated through a fixed random matrix instead of weights of the layers. Then network learns how to make this feedback useful \cite{n√∏kland2016direct}. Owing to this error propagation mechanism, DFA is considered to be more biologically plausible than BP and it opens the gate of parallelism in the training phase of ANNs.  
The problem at hand is known as the parity learning problem. Previous results showed that these parities are learnable by BP and lazy methods in a simpler setting whereas it is only learnable by BP in a more complex setting \cite{DBLP:journals/corr/abs-2002-07400}. That is why it is intriguing to test alternative algorithms on this problem to understand their learning dynamics and capabilities. \
The experiment results might lead us to three possible outcomes. First, we might acquire the similar performance as BP, if it is the case, it would be beneficial to test DFA and BP on a more challenging problem for further studies. Second, there might be a gap between BP and DFA, in this case, it would be intriguing to understand where the difference is coming from and how can we close this gap. Third, the alternative algorithm might not even learn and in this case, it is interesting to ask what makes a problem learnable by BP but not DFA. In all cases, results should help to understand the dynamics of learning of both methods. \
For applying BP and DFA in a more realistic setting, experiments are performed on the MNIST dataset by imitating the parity learning problem. After putting DFA to this frame, the reason behind the results is interpreted and possible improvements are motivated and implemented.\
Chapter \ref{chap:chapter_1} constructs the theoretical bases of the algorithms that are used for the experiments. These bases are composed of simple definitions, mathematical foundations, and the drawbacks of the algorithms. They are helpful to dig deeper into the learning structures of the training algorithms and, it is expected to have more control over their learning behaviors by tweaking components of these foundations. Also, it is beneficial to have these theoretical bases for acquiring a better understanding of the further interventions. Moreover, these theoretical foundations are used to implement the algorithms from scratch to use in experiments.\
Chapter \ref{chap:chapter_2} introduces the parity learning problem at hand. First, the formal definition of the problem is demonstrated then how the problem is imitated by using the MNIST dataset is explained in detail because this part is also highly correlated with the training phase of the algorithms.\
Chapter \ref{chap:chapter_3} presents results of the experiments. After having the same results from the previous studies for the parity problem, DFA is tested on the same problem. \
Chapter \ref{chap:chapter_4} wraps up the findings from experiments and creates a path for future studies. 