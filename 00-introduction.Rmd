---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Introduction {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Introduction}{}
```
\textbf{Artificial neural networks} (ANNs) are a collection of connected computational nodes inspired by biological neural networks. Each connection can transmit a helpful signal to another computational node like synapses in a brain. ANNs demonstrated colossal advancements in the last decades. Thanks to these advancements, it is possible to solve complex problems in computer vision, speech recognition, and natural language processing within a reasonable amount of time and with satisfactory performance. These advancements were actualized through an old but robust algorithm called \textbf{backpropagation} (BP). BP is a training algorithm for ANNs based on repeatedly adjusting network weights to minimize the difference (loss) between the output of the network and the ground truth \cite{Rumelhart:1986we}. In the training phase, the information from data is encoded via the minimization of the loss. After this phase, the network is ready to be tested on a validation dataset, and finally, it can be used to infer new information about the new data.  \
Although nowadays BP is the workhorse algorithm for training ANNs, it has some drawbacks, and it is not the only alternative. Recent studies offered different algorithms to train ANNs by addressing these drawbacks. These algorithms have other properties and principles than BP. Some of them are competitive with BP, or they even outperform it in terms of performance or convergence speed for specific problems. \
This thesis investigates the learning structures through BP and one of the alternative algorithm called \textbf{direct feedback alignment} (DFA) on the particular learning problems. Unlike BP, the error is propagated through a fixed random matrix parallelly instead of the layers' weights sequentially in DFA. Then network learns how to make this feedback useful \cite{n√∏kland2016direct}. Due to this error propagation mechanism, DFA is considered more biologically plausible than BP, and it opens the gate of parallelism in the training phase of ANNs.\
The fundamental interest of this study is in benchmarking the performance of BP and DFA on a given dataset. In other words, it is intriguing to compare the BP and DFA on a set of challenging learning problems. For this purpose, we consider two learning problems. The first learning problem we use is known as the \textbf{parity learning} problem. The parities are well-known to be difficult to learn for ANNs \cite{DBLP:journals/corr/abs-1807-06399}, and one of the previous studies showed that these parities are learnable by BP and linear methods in a more simple setting, whereas it is only learnable by BP in a more complex setting \cite{DBLP:journals/corr/abs-2002-07400}. The second learning problem is an example for a task where the inputs have a lower-dimensional structure \cite{chizat2020implicit}, and the number of dimensions of the data can be adjusted to decide the difficulty of the problem. Both learning problems proved the superiority of the BP compared to the linear methods, such as random features, which are sometimes referred to as lazy methods. That is why it is intriguing to test alternative algorithms on these problems to understand their learning dynamics and capabilities by observing the difference if there is any. \
The experiment results might lead us to three possible outcomes. First, we might acquire a similar performance as BP. It would be beneficial to test DFA and BP on a more challenging task if it is the case. Second, there might be a gap between BP and DFA then it would be intriguing to understand where the difference is coming from and how we can close this gap. Third, the alternative algorithm might not even learn, and in this case, it is interesting to ask what makes a problem learnable by BP but not DFA. In all cases, results should help understand both methods' learning dynamics. \
For applying BP and DFA in a more realistic setting, parity experiments are performed on the MNIST dataset by imitating the learning problem as it is described in \cite{DBLP:journals/corr/abs-2002-07400}. Then, they are also tested on the synthetic data \cite{chizat2020implicit} to compare the predictive power of BP and DFA. After putting DFA to these frames, the reason behind the results is interpreted, and possible improvements are motivated and implemented. \
Apart from the introduction and conclusion, this thesis comprises three chapters. For the fast readers, all of them have a section highlight. The content of them is explained as the following: \
Chapter \ref{chap:chapter_1} constructs the theoretical bases of the algorithms that are used for the experiments. These bases are composed of simple definitions, mathematical foundations, and the drawbacks of the algorithms. They help to dig deeper into the learning structures of the training algorithms. It is expected to have more control over their learning behaviors by adjusting components of these foundations. Also, it is beneficial to have these theoretical bases for acquiring a better understanding of the further interventions. Moreover, these theoretical foundations are used to implement the algorithms from scratch to use in experiments.\
Chapter \ref{chap:chapter_2} introduces the learning problems that we use. First, we start with the parity problem and continue with the synthetic data problem. We explained how their data are generated in detail for both problems and why they are interesting to test BP and DFA. This part is also highly correlated with the training phase of the algorithms. Therefore training phase and simple hyperparameter tuning process are explained in this chapter. Later the experiment results from previous studies \cite{DBLP:journals/corr/abs-2002-07400, chizat2020implicit}  are reproduced to have a concrete picture. \
Chapter \ref{chap:chapter_3} presents the results of the experiments. This chapter is the main contribution of this study. The first experiments test DFA on the same learning problems and compare with BP. As specified before, different further experiments are performed depending on the first experiment outcome. Such as closing the gap between BP and DFA, if any, and trying harder problems if we acquire similar results or explaining why DFA cannot learn the problem if we get a similar behavior as lazy methods. In addition to the main experiment, we perform other experiments because they might help us improve the performance of the algorithms, or they can be helpful to understand the learning dynamics of training algorithms. For instance, using different random matrices for DFA, trying adaptive optimization algorithms with BP and DFA, and for parity problem, observing hidden representation of BP and DFA to understand if the networks learn the digits individually to calculate the parities or capture different information.\
[Conclusion](#conc) refers to each chapter briefly and wraps up the outcomes of the experiments by summarizing the key findings. It gives a general idea about the comparison of DFA and BP. It also creates a path for future studies that are not covered in this study.