---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Learning Problems {#chap:chapter_2}
\minitoc <!-- this will include a mini table of contents-->

<!-- LaTeX normally does not indent the first line after a heading - however, it does so after the mini table of contents. You can manually tell it not to with \noindent -->

\noindent The success of neural networks spawned a great interest in comparing the predictive power of the various models. This involves testing different models on the same learning problem, usually difficult to learn, and observing which model performs better. It is particularly beneficial to understand their learning dynamics, which helps discover their limitations. These studies achieved striking success in finding out the superiority of the neural networks over linear methods. Given that, we are curious about the place of the DFA on these learning problems. For this purpose, we consider two learning problems: \textbf{parity learning} and \textbf{synthetic data problem}.

## Parity Learning Problem
The parities are notoriously hard to learn by ANNs \cite{DBLP:journals/corr/abs-1807-06399}. By taking advantage of this situation, Daniely and Malach \cite{DBLP:journals/corr/abs-2002-07400} questioned how far neural networks could go beyond the linear models. They did this by focusing on parities that have a complex family of target functions. They demonstrated that this family could be approximated by a two-layer network trained with Adadelta but not by lazy methods. This study brings an explanation of why neural networks' performance is better than linear methods, and it proves neural networks' learning capacities are beyond them.\
Experiments are performed on the MNIST dataset by imitating the parity problem. The task is: given a parameter k (defines the number of digits to be stacked together that is chosen uniformly from the dataset), determine if the sum of the digits is odd or even. When $k=1$, it is a simplified version of the standard MNIST task to find if a digit is even or odd. Experiment results showed that all models, including the lazy ones, reached a similar performance in the $k=1$ case where the neural network slightly outperformed others. On the other hand, the problem becomes more difficult for the case $k=3$ because models need to compute the parity of the digits' sum. In this case, there is a drastic gap between the neural network and lazy methods. Because the predictions of lazy methods did not go beyond the random guess.\
Since our goal is comparing DFA and BP on this particular problem, reproducing the results from \cite{DBLP:journals/corr/abs-2002-07400} is unavoidable. The same configurations are used with minor differences and, they are implemented in Pytorch \cite{NEURIPS2019_9015}. The network has only a hidden layer with $512$ neurons. For the last layer, sigmoid is used as a non-linear activation function. For the hidden layer, reLU is used. BCE is preferred as a loss function, and we have not used weight decay. In addition to previous settings, we have also used SGD to observe how much Adadelta improves. For the case $k=3$, we performed a simple hyper-parameter tuning process to get a decent learning rate for each method. Same learning rate values are used for the case $k=1$. The hyperparameter tuning process is the following. First, we define the parameter space, later we run with all different learning rates, and we compare these runs by the average of test accuracy of the last ten epochs, and we choose the highest one. It is also crucial to mention that, at each epoch, train data is recreated to boost the available data for the models. The same is also performed for test data to have an unbiased estimation of test accuracy. It implicitly prevents overfitting and increases the available data because the data creation is stochastic. The process is the following: random images are sampled uniformly from the MNIST dataset, then according to given parameter $k$, these random images are horizontally stacked. Hence we have different dataset at each epoch that helps networks to learn and perform better. Finally, these images are normalized before training the networks, which is necessary for deep learning tasks. 
```{r MNISTparity, fig.scap="Reproduced MNIST-Parity Experiment", fig.cap="\\textbf{Reproduced MNIST-Parity Experiment} \\newline Reproduced MNIST-Parity experiment from \\cite{DBLP:journals/corr/abs-2002-07400}. The top left is the test accuracy for the parity of a single image. The top right is the test accuracy for the parity of three images. The bottom is the examples from the dataset. The model needs to predict the parity of the sum of the digits." , fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/2_k13_SGD_ada_BP_reproduced.png")
```
\noindent The reproduced result can be observed in figure \ref{fig:MNISTparity}. Similar to results from \cite{DBLP:journals/corr/abs-2002-07400}, all the methods succeed learning for $k=1$ case. However, adadelta and SGD slightly outperformed lazy methods in this setting. In the case of $k=3$, adadelta and SGD almost reach $80\%$, but the performance of lazy methods does not go beyond a random guess.\ 
After having the concrete picture from the previous study, it is intriguing to see how DFA would perform with SGD and adaptive methods on this particular problem. We will investigate it in chapter \ref{chap:chapter_3} with other experiments.

## Synthetic Data Problem
Chizat and Bach \cite{chizat2020implicit} presented implicit bias in two-layer neural networks with cross-entropy loss trained with SGD. Implicit bias is a phenomenon that indicates that SGD is not only successful in finding the global minimum but also is biased towards solutions that generalize well \cite{yun2021unifying}. The study is beneficial to observe this phenomenon of gradient methods and training dynamics of wide neural networks. After demonstrating theoretical results, they performed numerical experiments to validate these results. The numerical experiments have a binary classification problem, and the data are synthetically generated. The problem is an example of a task where the inputs have a lower-dimensional structure. The number of samples and dimensions can be adjusted in the dataset. Similar to parity experiments, the results demonstrated the superiority of the neural network to the lazy method. Considering that all the nice properties of the dataset and the similarity of our previous comparison, it is an excellent problem to compare BP and DFA. Besides, it is nicer to compare BP and DFA in another challenging learning problem which the difficulty of it can be adjusted to avoid limiting our experiments with only the MNIST-Parity task.\
\noindent Given the parameter $c$ that denotes the number of clusters (in \cite{chizat2020implicit}, this parameter is denoted as $k$ since we used the same notation in parity problem, to avoid any confusion it is changed). The data is generated as the following: in dimension $d=2$, the distribution of the input values is a mixture of $c^2$ uniform distributions on the disk of radius $1/ (3c-1)$ on a uniform two-dimensional grid with step $3/(3c-1)$. Larger dimensions follow the uniform distribution  on $[-1/2,1/2]$. Each cluster is assigned randomly to a label \cite{chizat2020implicit}. In other words, after having the cluster centers, each input is sampled by following the uniform distribution with the shift angle and magnitude for the first two dimensions. Each input value is sampled from a uniform distribution on $[-1/2,1/2]$ for other spurious dimensions. Unlike the paper, labels are $0,1$, not $-1,1$ because it fits the structure of our previous architecture and training mechanism used for the parity experiment.\
Like the parity problem, before testing DFA on the problem, it is beneficial to reproduce results from the previous study. For this purpose, the scratch implementation is used with the same architecture (only a hidden layer, $512$ layer size, with reLU and sigmoid non-linearities). For each $n$ (number of training samples) and $d$ (number of dimensions, $\geq2$), experiments were performed three times, and they are plotted with a $95\%$ confidence interval. Similar to the paper, the learning rate is set to $0.5$ (no hyperparameter tuning process is performed for this problem), weight decay is not used, and the epoch number is $500$. We give enough time for all methods to converge with this high epoch number.
```{r randomData, fig.scap="Reproduced Synthetic Data Experiment", fig.cap="\\textbf{Reproduced Synthetic Data Experiment} \\newline Reproduced synthetic data experiment from \\cite{chizat2020implicit}. The networks are trained with backpropagation and SGD. On the left, distribution of the input on two first dimensions. In the middle, test error as a function of the number of inputs ($n$) with $d=15$. On the right, test error as a function of the number of dimensions ($d$) with $n=256$.", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/2_syntheticData_reproduced.png")
```
\noindent We can observe the reproduced experiment results in figure \ref{fig:randomData} from \cite{chizat2020implicit}. We have the data distribution in two-dimension on the left with $c=3$ (so we have 9 clusters) used for the experiment. This distribution is chosen because it is difficult to learn (it is not linearly separable). We have the test error and an increasing number of training samples in the middle plot where the dimension is $15$. We expect to observe decreasing test error with the increasing number of training samples for each model. On the right, we have the test error and the number of dimensions where the number of samples is $256$. The problem becomes challenging with the increasing number of dimensions because extracting useful information becomes more difficult for models with higher dimensional input since they are non-informative. Therefore we see the increasing test error. Similar to the paper results, we observe that training both layers gives better results than training only the output layer. In other words, increasing the number of training samples helps more to network than the lazy method, and the neural network is more successful in distinguishing useful inputs in high dimensions for this problem. It is important to mention that we have used only one distribution with $3$ repetition. The result may vary on distribution but on average, we should observe the superiority of BP as it is presented in \cite{chizat2020implicit}. It is interesting to put DFA in this frame to observe if it is closer to the lazy method or BP.\

\noindent \textbf{Highlights} \
After having the reproduced results for the synthetic data problem, we completed the chapter on learning problems.  First, we explained the problems and described how their data were generated and the task. Then we explained the details of the architectures of the networks, hyperparameter tuning process, and training phase used in the experiments. We presented the reproduced results from the papers and motivated the testing DFA on these problems.