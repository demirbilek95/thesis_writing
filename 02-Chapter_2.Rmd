---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Learning Problems {#chap:chapter_2}
\minitoc <!-- this will include a mini table of contents-->

<!-- LaTeX normally does not indent the first line after a heading - however, it does so after the mini table of contents. You can manually tell it not to with \noindent -->
\noindent The success of neural networks spawned a great interest in the community in the fields of learnability of the various models. This involves testing different models on the same problem and observing the results. It is particularly beneficial to understand the learning dynamics of the models, which helps to find out the limitations. These studies achieved striking success in understanding the neural networks.

## Parity Learning Problem
In \cite{DBLP:journals/corr/abs-2002-07400}, authors questioned how far neural networks could go beyond the linear models. They did this by focusing on parities that have a complex family of target functions. They demonstrated that this family could be approximated by a two-layer network trained with SGD, but not by lazy methods. This study brings an explanation of why neural networks' performance is better than linear methods, and it proves neural networks' learning capacities are beyond lazy methods.\
Experiments are performed on the MNIST dataset by imitating the parity problem. The task is: given the parameter k (defines the number of digits to be stacked together that is chosen uniformly from the dataset), determine if the sum of the digits is odd or even. When $k=1$, it is a simplified version of the standard MNIST task to find if a digit is even or odd, and experiment results showed that all models, including the lazy ones, reached a similar performance where the neural network slightly outperformed others. On the other hand, the problem becomes more difficult for the case $k=3$ because models need to compute the parity of the digits' sum. In this case, there is a drastic gap between the neural network and lazy methods.\
Since our goal is comparing DFA and BP on this particular problem, reproducing the results from \cite{DBLP:journals/corr/abs-2002-07400} is unavoidable. The same configurations are used from Pytorch \cite{NEURIPS2019_9015} with minor differences. The network has only a hidden layer with $512$ neurons. For the last layer, sigmoid is used as a non-linear activation function. For the hidden layer, reLU is used. BCE is preferred as a loss function, and $10^{-3}$ is set to weight decay. We also used SGD to observe how much Adadelta improves, and for the case $k=3$, we performed a simple hyper-parameter tuning process to get a decent learning rate for each method. Same values are used for the case $k=1$. The hyperparameter tuning process is the following. First, we define the parameter space, later we run with all different learning rates, and we compare these runs by the average of test accuracy of the last ten epochs, and we choose the highest one. It is also crucial to mention that, at each epoch, train data is recreated to boost the available data for the models. The same is also performed for test data to have an unbiased estimation of test accuracy.

```{r MNISTparity, fig.cap="Reproduced MNIST parity experiment result from \\cite{DBLP:journals/corr/abs-2002-07400}", fig.align='center', out.width='100%', echo=FALSE}
knitr::include_graphics("figures/reproduce.png")
```
\noindent The reproduced result can be observed in figure \ref{fig:MNISTparity}. Similar to results from \cite{DBLP:journals/corr/abs-2002-07400}, all the methods succeed learning for $k=1$ case. However, adadelta and SGD slightly outperformed lazy methods in this setting. In the case of $k=3$, adadelta and SGD almost reach $80\%$, but the performance of lazy methods doesn't go beyond a random guess.\ 
After having the concrete picture from the previous study, it is intriguing to see how DFA would perform with SGD and adaptive methods on this particular problem. We will investigate it in chapter \ref{chap:chapter_3} with other experiments.

## Random Data 
TODO: Write how this data generated, why it's interesting and why it is suitable for testing learning dynamics of the methods.
